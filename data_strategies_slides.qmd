---
title: "Data Strategies for Future Us"
author: "Andy P. Barrett,\n National Snow and Ice Data Center"
date: last-modified
date-format: iso
format:
    revealjs:
        colorlinks: true
        theme: night
        
---

## What are Data Strategies?

Data Strategies enhance collaboration and reproducible science.


 - Workflows;  
 - Data management best practices;  
 - Documentation;  


Good to start from the beginning of a project, great to start
from where you now.


## Who is future us?

:::: {.columns}  
::: {.column}  
You.

Your team.

The scientific community.
:::  

::: {.column}
![](images/starwars-teamwork.png)
:::
::::

## A simple data workflow
![http://r4ds.hadley.nz/](images/data-science-workflow.png)


## When to cloud?

 - What is the data volume?
 - How long will it take to download?
 - Can you store all that data (cost and space)?  
 - Do you have the computing power for processing?  
 - Does your team need a common computing environment?  
 - Do you need to share data at each step or just an end product?  


## Solutions
```{dot -Tpng}
digraph LocalWorkflow {
  fontname="Helvetica,Arial,sans-serif"
  cloud [image="images/cloud_image.png", label="EarthData Cloud"];
  tidy [shape=plaintext, label="Tidy"];
  transform [shape=plaintext, label="Transform"];
  visualize [shape=plaintext, label="Visualize"];
  model [shape=plaintext, label="Model"];
  communicate [shape=plaintext, label="Communicate"];
  
  cloud -> tidy;

  subgraph LaptopCluster {
      label="Local Machine";
      style=filled;
      color=lightgrey;
      tidy -> transform -> visualize -> model -> transform;
  }
      
}
```
  
Add cloud and other workflow diagrams

 - Download data to Local Machine and process locally
 - Do some processing in cloud to reduce data size
 - Do all of your processing in the cloud


## Test Diagram
```{dot}
digraph G {
	fontname="Helvetica,Arial,sans-serif"
	node [fontname="Helvetica,Arial,sans-serif"]
	edge [fontname="Helvetica,Arial,sans-serif"]

	subgraph cluster_0 {
		style=filled;
		color=lightgrey;
		node [shape=plaintext, style=filled,color=white];
		Tidy -> Transform -> Visualize -> Model -> Transform;
		label = "Local Machine";
	}

        cloud -> Tidy [label="Import", labelfloat=true];
        Model -> communicate;
        
	communicate [shape=plaintext, label="Communicate"];
        cloud [shape=plaintest, label="Cloud"];
}
```
## What does this look like in the cloud?

*Placeholder for examples of hybrid and cloud workflows*


## How to future-proof workflows and make them reproducible

**FAIR**

 - **F**indable,
 - **A**ccessible,
 - **I**nteroperable,
 - **R**eusable

Applies to the future you and your team as well.


## Make sure data are **F**indable and **A**ccessible

Does everyone on your team know where the data is?

Can they access it?  

Helpful to document this somewhere.


## Data Management

Keep raw data, raw!

Save intermediate data not just final versions.

Use consistent and descriptive folder and file name patterns.

```
(base) nsidc-442-abarrett:data_strategies_for_a_future_us$ tree Data
Data
├── calibrated
├── cleaned
├── figures
├── final
├── monthly_averages
├── raw
└── results

7 directories, 0 files
```

## Standard file formats make data **I**nteroperable

 - GeoTIFF for imagery or 2D raster data   
 - NetCDF for multi-dimensional data (3D+)  
 - Shapefiles or GeoJSON for vector data  
 - csv for tabular data.

Avoid Excel and other proprietary formats.


## Metadata makes data **I**nteroperable and **R**eusable.  

Metadata standards and conventions ensure that standard tools can
read/interpret the data.

Standards also define the meaning of metadata attributes.

 - What is the Coordinate Reference System?  
   (projection, grid mapping)
 - What are the units?  
 - What is the variable name?  
 - What is the source of the data?  
 - What script produced the data?  


## Document the Analysis

Document each step.

 - Where did you get the data, which files, which version?
 - Write it down.  Anywhere is good but using a script is better.

Can you (or anyone else) easily reproduce your processing pipeline?

With GUI interfaces - e.g. ArcGIS, QGIS, Excel - use screenshots, journal commands.
